{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original edge_index shape: torch.Size([2, 8])\n",
      "tensor([[0, 1, 1, 2, 2, 3, 3, 0],\n",
      "        [1, 0, 2, 1, 3, 2, 0, 3]])\n",
      "\n",
      "After dropout:\n",
      "Dropped edge_index shape: torch.Size([2, 4])\n",
      "tensor([[0, 0, 1, 3],\n",
      "        [1, 3, 0, 0]])\n",
      "\n",
      "edge_id (which original undirected pairs were kept):\n",
      "tensor([0, 7, 0, 7])\n",
      "\n",
      "After dropout:\n",
      "Dropped edge_index shape: torch.Size([2, 6])\n",
      "tensor([[0, 1, 2, 1, 2, 3],\n",
      "        [1, 2, 3, 0, 1, 2]])\n",
      "\n",
      "edge_id (which original undirected pairs were kept):\n",
      "tensor([0, 2, 4, 0, 2, 4])\n",
      "\n",
      "After dropout:\n",
      "Dropped edge_index shape: torch.Size([2, 6])\n",
      "tensor([[0, 1, 0, 1, 2, 3],\n",
      "        [1, 2, 3, 0, 1, 0]])\n",
      "\n",
      "edge_id (which original undirected pairs were kept):\n",
      "tensor([0, 2, 7, 0, 2, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import dropout_edge\n",
    "\n",
    "# make a simple undirected edge_index with both directions\n",
    "edge_index = torch.tensor([\n",
    "    [0, 1, 1, 2, 2, 3, 3, 0],  # sources\n",
    "    [1, 0, 2, 1, 3, 2, 0, 3]   # targets\n",
    "], dtype=torch.long)\n",
    "\n",
    "print(\"Original edge_index shape:\", edge_index.shape)\n",
    "print(edge_index)\n",
    "\n",
    "# drop 50% of undirected edges (both directions together)\n",
    "# torch.manual_seed(42)  # for reproducibility\n",
    "\n",
    "for i in range(3):\n",
    "    edge_index_dropped, edge_id = dropout_edge(\n",
    "        edge_index, p=0.5, force_undirected=True, training=True\n",
    "    )\n",
    "\n",
    "    print(\"\\nAfter dropout:\")\n",
    "    print(\"Dropped edge_index shape:\", edge_index_dropped.shape)\n",
    "    print(edge_index_dropped)\n",
    "\n",
    "    print(\"\\nedge_id (which original undirected pairs were kept):\")\n",
    "    print(edge_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 2, 2, 3, 3, 0],\n",
       "        [1, 0, 2, 1, 3, 2, 0, 3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIG ===\n",
      "base_dir: /disk/10tb/home/shmelev/simulated_dataset_for_workshop\n",
      "seed_csv: /disk/10tb/home/shmelev/cr_only/CR.csv\n",
      "================\n",
      "\n",
      "[INFO] base_dir already exists: /disk/10tb/home/shmelev/simulated_dataset_for_workshop\n",
      "\n",
      "[1/5] Loading CR dataset and computing simulation parameters...\n",
      "[OK] Computed parameters for CR dataset.\n",
      "Shapes:\n",
      "  edge_probs: (4, 4)\n",
      "  mean_weight: (4, 4)\n",
      "Applying np.nan_to_num on edge_probs (no assignment, same style as original).\n",
      "Class counts (CR):\n",
      "0    1586\n",
      "2    1582\n",
      "1     792\n",
      "3     673\n",
      "Name: class_id, dtype: int64\n",
      "Classes (dp): ['Southern Russians', 'Ukranians', 'Northen Russians', 'Belarusians'] \n",
      "\n",
      "[2/5] Preparing population sizes to resemble the real dataset (no modifications).\n",
      "pop_sizes: [1586, 792, 1582, 673]\n",
      "Total individuals (base): 4633\n",
      "\n",
      "[3/5] Simulating synthetic dataset with NullSimulator (unchanged probs/weights, CR-like).\n",
      "[OK] NullSimulator initialized with 4 classes.\n",
      "[OK] Generated matrices from NullSimulator.\n",
      "  means shape: (4633, 4633)\n",
      "  counts shape: (4633, 4633)\n",
      "  pop_index sample (first 20): [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[OK] Simulated dataset saved to: /disk/10tb/home/shmelev/simulated_dataset_for_workshop/simulated_dataset_cr.csv\n",
      "\n",
      "[4/5] Building train/valid/test graphs with GENLINK DataProcessor (one_hot features)...\n",
      "[OK] DataProcessor created for simulated CR-like dataset.\n",
      "100.0% (4633) of all nodes in dataset were used to create splits (no masked nodes assumed)\n",
      "[OK] Generated random train/valid/test node splits.\n",
      "[OK] Constructed train/valid/test datasets (one_hot, homogeneous, one/multiple).\n",
      "Counts summary:\n",
      "  Training graphs: 2782\n",
      "  Validation graphs: 926\n",
      "  Test graphs: 925\n",
      "First training graph stats:\n",
      "  x: (2782, 4) y: (2782,) edge_index: (2, 58030) weight: (58030,)\n",
      "First validation graph stats:\n",
      "  x: (2783, 4) y: (2783,) edge_index: (2, 58060) weight: (58060,)\n",
      "First test graph stats (before masking):\n",
      "  x: (2783, 4) y: (2783,) edge_index: (2, 58066) weight: (58066,)\n",
      "  last-node true label (will be hidden): 3\n",
      "\n",
      "[5/5] Saving training/validation graphs, masking test graphs, writing CSV mapping and sample submission...\n",
      "[OK] Saved training graphs to: /disk/10tb/home/shmelev/simulated_dataset_for_workshop/train_graphs.pickle\n",
      "[OK] Saved validation graphs to: /disk/10tb/home/shmelev/simulated_dataset_for_workshop/validation_graphs.pickle\n",
      "  [mask] graph_0: true_label=3 -> y[-1] set to -1\n",
      "  [mask] graph_1: true_label=1 -> y[-1] set to -1\n",
      "  [mask] graph_2: true_label=3 -> y[-1] set to -1\n",
      "  [mask] graph_3: true_label=3 -> y[-1] set to -1\n",
      "  [mask] graph_4: true_label=1 -> y[-1] set to -1\n",
      "[OK] Saved masked test graphs to: /disk/10tb/home/shmelev/simulated_dataset_for_workshop/test_graphs.pickle\n",
      "[OK] Wrote test labels CSV to: /disk/10tb/home/shmelev/simulated_dataset_for_workshop/test_labels.csv\n",
      "[OK] Wrote sample submission file to: /disk/10tb/home/shmelev/simulated_dataset_for_workshop/sample_submission.csv\n",
      "\n",
      "=== SUMMARY ===\n",
      "Train graphs: 2782  -> /disk/10tb/home/shmelev/simulated_dataset_for_workshop/train_graphs.pickle\n",
      "Valid graphs: 926 -> /disk/10tb/home/shmelev/simulated_dataset_for_workshop/validation_graphs.pickle\n",
      "Test  graphs: 925            -> /disk/10tb/home/shmelev/simulated_dataset_for_workshop/test_graphs.pickle\n",
      "Test labels CSV preview (first 6 lines):\n",
      "sample,label\n",
      "graph_0,3\n",
      "graph_1,1\n",
      "graph_2,3\n",
      "graph_3,3\n",
      "graph_4,1\n",
      "\n",
      "Sample submission preview (first 6 lines):\n",
      "ID\tPREDICTED_CLASS\n",
      "0\t0\n",
      "1\t0\n",
      "2\t0\n",
      "3\t0\n",
      "4\t0\n",
      "All done (CR dataset; one_hot features; no modifications to edge/weight probabilities).\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# JUPYTER NOTEBOOK: DATASET SIMULATION & EXPORT (PICKLE)\n",
    "# =========================\n",
    "\n",
    "# --- Imports and setup ---\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# keep this exact sys.path append (mandatory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(''), os.path.pardir)))\n",
    "\n",
    "from utils.genlink import DataProcessor, NullSimulator  # datasets only, no neural nets\n",
    "\n",
    "# --- Paths ---\n",
    "base_dir = '/disk/10tb/home/shmelev/simulated_dataset_for_workshop'\n",
    "seed_csv = '/disk/10tb/home/shmelev/cr_only/CR.csv'\n",
    "\n",
    "print(\"=== CONFIG ===\")\n",
    "print(f\"base_dir: {base_dir}\")\n",
    "print(f\"seed_csv: {seed_csv}\")\n",
    "print(\"================\\n\")\n",
    "\n",
    "# ensure output dir exists\n",
    "if not os.path.exists(base_dir):\n",
    "    print(f\"[INFO] Creating base_dir: {base_dir}\")\n",
    "    os.makedirs(base_dir)\n",
    "else:\n",
    "    print(f\"[INFO] base_dir already exists: {base_dir}\")\n",
    "\n",
    "# --- 1) Load reference dataset (CR) and compute simulation parameters ---\n",
    "print(\"\\n[1/5] Loading CR dataset and computing simulation parameters...\")\n",
    "dp = DataProcessor(seed_csv)\n",
    "dp.compute_simulation_params()\n",
    "print(\"[OK] Computed parameters for CR dataset.\")\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  edge_probs:\", dp.edge_probs.shape)\n",
    "print(\"  mean_weight:\", dp.mean_weight.shape)\n",
    "print(\"Applying np.nan_to_num on edge_probs (no assignment, same style as original).\")\n",
    "np.nan_to_num(dp.edge_probs)\n",
    "\n",
    "print(\"Class counts (CR):\")\n",
    "print(dp.node_classes_sorted.iloc[:, 1].value_counts())\n",
    "print(\"Classes (dp):\", dp.classes, \"\\n\")\n",
    "\n",
    "# --- 2) Prepare population sizes (NO modifications to probabilities/weights) ---\n",
    "print(\"[2/5] Preparing population sizes to resemble the real dataset (no modifications).\")\n",
    "pop_sizes = []\n",
    "for i in range(len(dp.classes)):\n",
    "    pop_sizes.append(dp.node_classes_sorted.iloc[:, 1].value_counts().loc[i])\n",
    "print(\"pop_sizes:\", pop_sizes)\n",
    "print(\"Total individuals (base):\", sum(pop_sizes))\n",
    "\n",
    "# --- 3) Simulate synthetic dataset that resembles the original (no tweaks, no multipliers) ---\n",
    "print(\"\\n[3/5] Simulating synthetic dataset with NullSimulator (unchanged probs/weights, CR-like).\")\n",
    "ns = NullSimulator(len(dp.classes), np.nan_to_num(dp.edge_probs), dp.mean_weight)\n",
    "print(f\"[OK] NullSimulator initialized with {len(dp.classes)} classes.\")\n",
    "\n",
    "means, counts, pop_index = ns.generate_matrices(np.array(pop_sizes), np.random.default_rng(42))\n",
    "print(\"[OK] Generated matrices from NullSimulator.\")\n",
    "print(\"  means shape:\", means.shape)\n",
    "print(\"  counts shape:\", counts.shape)\n",
    "print(\"  pop_index sample (first 20):\", pop_index[:20])\n",
    "\n",
    "simulated_csv = os.path.join(base_dir, 'simulated_dataset_cr.csv')\n",
    "ns.simulate_graph(means, counts, pop_index, simulated_csv)\n",
    "print(f\"[OK] Simulated dataset saved to: {simulated_csv}\")\n",
    "\n",
    "# --- 4) Build train/valid/test graphs (ONE-HOT features), save pickles, mask test labels, write CSV map ---\n",
    "print(\"\\n[4/5] Building train/valid/test graphs with GENLINK DataProcessor (one_hot features)...\")\n",
    "dataset = DataProcessor(simulated_csv)\n",
    "print(\"[OK] DataProcessor created for simulated CR-like dataset.\")\n",
    "\n",
    "dataset.generate_random_train_valid_test_nodes(train_size=0.6, \n",
    "                                               valid_size=0.2, \n",
    "                                               test_size=0.2, \n",
    "                                               random_state=42,\n",
    "                                               save_dir=None, \n",
    "                                               mask_size=None,\n",
    "                                               sub_train_size=None, \n",
    "                                               keep_train_nodes=None, \n",
    "                                               mask_random_state=None)\n",
    "print(\"[OK] Generated random train/valid/test node splits.\")\n",
    "\n",
    "# Use ONE-HOT features (changed from graph_based)\n",
    "dataset.make_train_valid_test_datasets_with_numba('one_hot',\n",
    "                                                  'homogeneous',\n",
    "                                                  'multiple',\n",
    "                                                  'multiple',\n",
    "                                                  'simulated_dataset',\n",
    "                                                  masking=False,\n",
    "                                                  no_mask_class_in_df=True)\n",
    "print(\"[OK] Constructed train/valid/test datasets (one_hot, homogeneous, one/multiple).\")\n",
    "\n",
    "print(\"Counts summary:\")\n",
    "print(\"  Training graphs:\", len(dataset.array_of_graphs_for_training))\n",
    "print(\"  Validation graphs:\", len(dataset.array_of_graphs_for_validation))\n",
    "print(\"  Test graphs:\", len(dataset.array_of_graphs_for_testing))\n",
    "\n",
    "# peek at one example per split if present\n",
    "if len(dataset.array_of_graphs_for_training) > 0:\n",
    "    g0 = dataset.array_of_graphs_for_training[0]\n",
    "    print(\"First training graph stats:\")\n",
    "    print(\"  x:\", tuple(g0.x.shape), \"y:\", tuple(g0.y.shape),\n",
    "          \"edge_index:\", tuple(g0.edge_index.shape), \"weight:\", tuple(g0.weight.shape))\n",
    "if len(dataset.array_of_graphs_for_validation) > 0:\n",
    "    g1 = dataset.array_of_graphs_for_validation[0]\n",
    "    print(\"First validation graph stats:\")\n",
    "    print(\"  x:\", tuple(g1.x.shape), \"y:\", tuple(g1.y.shape),\n",
    "          \"edge_index:\", tuple(g1.edge_index.shape), \"weight:\", tuple(g1.weight.shape))\n",
    "if len(dataset.array_of_graphs_for_testing) > 0:\n",
    "    g2 = dataset.array_of_graphs_for_testing[0]\n",
    "    print(\"First test graph stats (before masking):\")\n",
    "    print(\"  x:\", tuple(g2.x.shape), \"y:\", tuple(g2.y.shape),\n",
    "          \"edge_index:\", tuple(g2.edge_index.shape), \"weight:\", tuple(g2.weight.shape))\n",
    "    print(\"  last-node true label (will be hidden):\", int(g2.y[-1]))\n",
    "\n",
    "# Save pickles\n",
    "print(\"\\n[5/5] Saving training/validation graphs, masking test graphs, writing CSV mapping and sample submission...\")\n",
    "train_pkl = os.path.join(base_dir, 'train_graphs.pickle')\n",
    "val_pkl = os.path.join(base_dir, 'validation_graphs.pickle')\n",
    "test_pkl = os.path.join(base_dir, 'test_graphs.pickle')\n",
    "\n",
    "with open(train_pkl, 'wb') as handle:\n",
    "    pickle.dump(dataset.array_of_graphs_for_training, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"[OK] Saved training graphs to: {train_pkl}\")\n",
    "\n",
    "with open(val_pkl, 'wb') as handle:\n",
    "    pickle.dump(dataset.array_of_graphs_for_validation, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"[OK] Saved validation graphs to: {val_pkl}\")\n",
    "\n",
    "# Mask the specific node in every test graph and write mapping CSV\n",
    "masked_test_graphs = []\n",
    "lines = [\"sample,label\\n\"]\n",
    "\n",
    "for i in range(len(dataset.array_of_graphs_for_testing)):\n",
    "    g = dataset.array_of_graphs_for_testing[i]\n",
    "    true_label = int(g.y[-1])\n",
    "    sample_name = f\"graph_{i}\"\n",
    "    lines.append(f\"{sample_name},{true_label}\\n\")\n",
    "    g.y[-1] = -1  # hide the label\n",
    "    masked_test_graphs.append(g)\n",
    "    if i < 5:\n",
    "        print(f\"  [mask] {sample_name}: true_label={true_label} -> y[-1] set to -1\")\n",
    "\n",
    "with open(test_pkl, 'wb') as handle:\n",
    "    pickle.dump(masked_test_graphs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"[OK] Saved masked test graphs to: {test_pkl}\")\n",
    "\n",
    "test_csv = os.path.join(base_dir, 'test_labels.csv')\n",
    "with open(test_csv, 'w') as f:\n",
    "    for line in lines:\n",
    "        f.write(line)\n",
    "print(f\"[OK] Wrote test labels CSV to: {test_csv}\")\n",
    "\n",
    "# Generate a sample submission file (tab-separated, as discussed)\n",
    "sample_sub_csv = os.path.join(base_dir, 'sample_submission.csv')\n",
    "with open(sample_sub_csv, 'w') as f:\n",
    "    f.write(\"ID\\tPREDICTED_CLASS\\n\")\n",
    "    for i in range(len(dataset.array_of_graphs_for_validation)):\n",
    "        f.write(f\"{i}\\t0\\n\")  # dummy class 0 for all, demonstrates required format\n",
    "print(f\"[OK] Wrote sample submission file to: {sample_sub_csv}\")\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"Train graphs: {len(dataset.array_of_graphs_for_training)}  -> {train_pkl}\")\n",
    "print(f\"Valid graphs: {len(dataset.array_of_graphs_for_validation)} -> {val_pkl}\")\n",
    "print(f\"Test  graphs: {len(masked_test_graphs)}            -> {test_pkl}\")\n",
    "print(f\"Test labels CSV preview (first 6 lines):\\n{''.join(lines[:6])}\")\n",
    "print(\"Sample submission preview (first 6 lines):\")\n",
    "with open(sample_sub_csv, 'r') as f:\n",
    "    for k, line in enumerate(f):\n",
    "        if k > 5:\n",
    "            break\n",
    "        print(line.rstrip())\n",
    "print(\"All done (CR dataset; one_hot features; no modifications to edge/weight probabilities).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genlink",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
